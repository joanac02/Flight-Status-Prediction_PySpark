{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração Inicial e Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicialização da sessão Spark\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Projeto_AJP\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.driver.memory\", \"8g\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\", False)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados de teste\n",
    "test_data = spark.read.parquet(\"../data/processed/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento dos Modelos Treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os modelos treinados\n",
    "from pyspark.ml.classification import LogisticRegressionModel, DecisionTreeClassificationModel, RandomForestClassificationModel, GBTClassificationModel\n",
    "\n",
    "lr_model = LogisticRegressionModel.load(\"../models/logistic_regression_model\")\n",
    "dt_model = DecisionTreeClassificationModel.load(\"../models/decision_tree_model\")\n",
    "rf_model = RandomForestClassificationModel.load(\"../models/random_forest_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleção de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os avaliadores\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"IsDelayed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"IsDelayed\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"IsDelayed\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"IsDelayed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "roc_auc_evaluator = BinaryClassificationEvaluator(labelCol=\"IsDelayed\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular as métricas\n",
    "def compute_metrics(predictions):\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    precision = precision_evaluator.evaluate(predictions)\n",
    "    recall = recall_evaluator.evaluate(predictions)\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    roc_auc = roc_auc_evaluator.evaluate(predictions)\n",
    "    return accuracy, roc_auc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.615651429194504\n",
      "ROC AUC: 0.6541888094853675\n",
      "Precision: 0.6570287089829441\n",
      "Recall: 0.6156514291945041\n",
      "F1 Score: 0.6272727447484185\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_data)\n",
    "lr_metrics = compute_metrics(lr_predictions)\n",
    "\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {lr_metrics[0]}\")\n",
    "print(f\"ROC AUC: {lr_metrics[1]}\")\n",
    "print(f\"Precision: {lr_metrics[2]}\")\n",
    "print(f\"Recall: {lr_metrics[3]}\")\n",
    "print(f\"F1 Score: {lr_metrics[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Metrics:\n",
      "Accuracy: 0.633772944338641\n",
      "ROC AUC: 0.5163442885959006\n",
      "Precision: 0.6535648227382512\n",
      "Recall: 0.6337729443386408\n",
      "F1 Score: 0.6412064129527568\n"
     ]
    }
   ],
   "source": [
    "dt_predictions = dt_model.transform(test_data)\n",
    "dt_metrics = compute_metrics(dt_predictions)\n",
    "\n",
    "print(\"\\nDecision Tree Metrics:\")\n",
    "print(f\"Accuracy: {dt_metrics[0]}\")\n",
    "print(f\"ROC AUC: {dt_metrics[1]}\")\n",
    "print(f\"Precision: {dt_metrics[2]}\")\n",
    "print(f\"Recall: {dt_metrics[3]}\")\n",
    "print(f\"F1 Score: {dt_metrics[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Metrics:\n",
      "Accuracy: 0.6086839229336289\n",
      "ROC AUC: 0.6352610490166465\n",
      "Precision: 0.6448718910406317\n",
      "Recall: 0.6086839229336288\n",
      "F1 Score: 0.6198581541931213\n"
     ]
    }
   ],
   "source": [
    "rf_predictions = rf_model.transform(test_data)\n",
    "rf_metrics = compute_metrics(rf_predictions)\n",
    "\n",
    "print(\"\\nRandom Forest Metrics:\")\n",
    "print(f\"Accuracy: {rf_metrics[0]}\")\n",
    "print(f\"ROC AUC: {rf_metrics[1]}\")\n",
    "print(f\"Precision: {rf_metrics[2]}\")\n",
    "print(f\"Recall: {rf_metrics[3]}\")\n",
    "print(f\"F1 Score: {rf_metrics[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar as previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions saved.\n"
     ]
    }
   ],
   "source": [
    "lr_predictions.write.mode(\"overwrite\").parquet(\"../data/predictions/lr_predictions.parquet\")\n",
    "dt_predictions.write.mode(\"overwrite\").parquet(\"../data/predictions/dt_predictions.parquet\")\n",
    "rf_predictions.write.mode(\"overwrite\").parquet(\"../data/predictions/rf_predictions.parquet\")\n",
    "\n",
    "print(\"Model predictions saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encerrar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
